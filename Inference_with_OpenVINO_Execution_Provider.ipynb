{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R-Mahmoudi/Inference_with_OpenVINO_Execution_Provider/blob/main/Inference_with_OpenVINO_Execution_Provider.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mobilenet V2 Inference with OpenVINOâ„¢ Execution Provider for ONNX Runtime on CPU**"
      ],
      "metadata": {
        "id": "9-nQ5dau044q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's install the necessary packages. We will install PyTorch, onnxruntime-openvino 1.11, ONNX and pillow.**"
      ],
      "metadata": {
        "id": "RLntm-8w1yis"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0j7I2wetpW7",
        "outputId": "0a0931e4-01f5-4272-8174-2234f82121a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.1+cu116)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (0.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.25.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (9.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.8/dist-packages (1.12.0)\n",
            "Collecting onnx\n",
            "  Using cached onnx-1.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "Collecting protobuf<4,>=3.20.2\n",
            "  Using cached protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx) (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from onnx) (1.21.6)\n",
            "Installing collected packages: protobuf, onnx\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.1\n",
            "    Uninstalling protobuf-3.20.1:\n",
            "      Successfully uninstalled protobuf-3.20.1\n",
            "  Attempting uninstall: onnx\n",
            "    Found existing installation: onnx 1.12.0\n",
            "    Uninstalling onnx-1.12.0:\n",
            "      Successfully uninstalled onnx-1.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "onnxruntime-openvino 1.13.1 requires protobuf<=3.20.1,>=3.12.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed onnx-1.13.0 protobuf-3.20.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pillow==9.0.0 in /usr/local/lib/python3.8/dist-packages (9.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: onnxruntime-openvino in /usr/local/lib/python3.8/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from onnxruntime-openvino) (1.21.6)\n",
            "Requirement already satisfied: cerberus in /usr/local/lib/python3.8/dist-packages (from onnxruntime-openvino) (1.3.4)\n",
            "Collecting protobuf<=3.20.1,>=3.12.2\n",
            "  Using cached protobuf-3.20.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from onnxruntime-openvino) (3.1.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime-openvino) (1.12)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.8/dist-packages (from onnxruntime-openvino) (1.13.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from onnxruntime-openvino) (23.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from onnxruntime-openvino) (1.7.1)\n",
            "Requirement already satisfied: setuptools>=41.4.0 in /usr/local/lib/python3.8/dist-packages (from onnxruntime-openvino) (57.4.0)\n",
            "Collecting onnx\n",
            "  Using cached onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx->onnxruntime-openvino) (4.4.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->onnxruntime-openvino) (1.2.1)\n",
            "Installing collected packages: protobuf, onnx\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: onnx\n",
            "    Found existing installation: onnx 1.13.0\n",
            "    Uninstalling onnx-1.13.0:\n",
            "      Successfully uninstalled onnx-1.13.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "googleapis-common-protos 1.58.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-translate 3.8.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-language 2.6.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-firestore 2.7.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-datastore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery 3.4.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.18.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed onnx-1.12.0 protobuf-3.20.1\n"
          ]
        }
      ],
      "source": [
        "! pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/torch_stable.html\n",
        "! pip install --upgrade onnx\n",
        "!pip install  pillow==9.0.0\n",
        "! pip install onnxruntime-openvino"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use torchvision provided API to load mobilenet_v2 model.**"
      ],
      "metadata": {
        "id": "TPLeHpQ-1s37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models, datasets, transforms as T\n",
        "mobilenet_v2 = models.mobilenet_v2(pretrained=True)"
      ],
      "metadata": {
        "id": "YbNu1CM6ujNA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83073e97-0026-46c7-c28e-c73f3a840f4b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pytorch onnx export API to export the model.**"
      ],
      "metadata": {
        "id": "zHFS4xb324dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "image_height = 224\n",
        "image_width = 224\n",
        "x = torch.randn(1, 3, image_height, image_width, requires_grad=True)\n",
        "torch_out = mobilenet_v2(x)\n",
        "\n",
        "# Export the model\n",
        "torch.onnx.export(mobilenet_v2,              # model being run\n",
        "                  x,                         # model input (or a tuple for multiple inputs)\n",
        "                  \"mobilenet_v2_float.onnx\", # where to save the model (can be a file or file-like object)\n",
        "                  export_params=True,        # store the trained parameter weights inside the model file\n",
        "                  opset_version=12,          # the ONNX version to export the model to\n",
        "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "                  input_names = ['input'],   # the model's input names\n",
        "                  output_names = ['output']) # the model's output names"
      ],
      "metadata": {
        "id": "nt1lz8OGvNTN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run an sample with the FP32 ONNX model. Firstly, implement the preprocess.**"
      ],
      "metadata": {
        "id": "_0KAfxdC29uS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import onnxruntime\n",
        "import torch\n",
        "\n",
        "def preprocess_image(image_path, height, width, channels=3):\n",
        "    image = Image.open(image_path)\n",
        "    image = image.resize((width, height), Image.ANTIALIAS)\n",
        "    image_data = np.asarray(image).astype(np.float32)\n",
        "    image_data = image_data.transpose([2, 0, 1]) # transpose to CHW\n",
        "    mean = np.array([0.079, 0.05, 0]) + 0.406\n",
        "    std = np.array([0.005, 0, 0.001]) + 0.224\n",
        "    for channel in range(image_data.shape[0]):\n",
        "        image_data[channel, :, :] = (image_data[channel, :, :] / 255 - mean[channel]) / std[channel]\n",
        "    image_data = np.expand_dims(image_data, 0)\n",
        "    return image_data"
      ],
      "metadata": {
        "id": "1DGDgz07vRr0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e50e104-ae19-488d-c9c0-d03dcaff9157"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download the imagenet labels and load it.**"
      ],
      "metadata": {
        "id": "DlC5Q_zN3EG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download ImageNet labels\n",
        "!curl -o imagenet_classes.txt https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
        "\n",
        "# Read the categories\n",
        "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
        "    categories = [s.strip() for s in f.readlines()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEtu0GSjvUPn",
        "outputId": "cff87f68-3d80-430a-8bff-1bda6ccc3656"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 10472  100 10472    0     0  83111      0 --:--:-- --:--:-- --:--:-- 83776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o cat.jpg https://raw.githubusercontent.com/maxogden/cat-picture/master/cat.jpg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9Z-gqF35DYJ",
        "outputId": "1153f2e9-d03b-4615-96f8-2cde8fdc39b9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 78044  100 78044    0     0   399k      0 --:--:-- --:--:-- --:--:--  399k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run the example with OpenVINO Execution Provider for ONNX Runtime on CPU.**"
      ],
      "metadata": {
        "id": "SyD370d96be3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "#set the provider as OpenVINO and device as CPU\n",
        "session_openvino = onnxruntime.InferenceSession(\"mobilenet_v2_float.onnx\",providers=['OpenVINOExecutionProvider'], provider_options=[{'device_type' : 'CPU_FP32'}])\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "def run_sample(session, image_file, categories):\n",
        "    output = session.run([], {'input':preprocess_image(image_file, image_height, image_width)})[0]\n",
        "    output = output.flatten()\n",
        "    output = softmax(output) # this is optional\n",
        "    top5_catid = np.argsort(-output)[:5]\n",
        "    for catid in top5_catid:\n",
        "        print(categories[catid], output[catid])\n",
        "start = time.time()\n",
        "run_sample(session_openvino, 'cat.jpg', categories)\n",
        "elapsed = time.time() - start\n",
        "print('Inference time in ms: %f' % (elapsed * 1000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YJYmrqk6A3Z",
        "outputId": "49ed0ed3-c285-4d03-ec55-48398c2a34bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tabby 0.3298738\n",
            "Persian cat 0.17306244\n",
            "tiger cat 0.16069515\n",
            "lynx 0.12683442\n",
            "Egyptian cat 0.07306996\n",
            "Inference time in ms: 50.282240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GXDfKb9Yw1-J"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}